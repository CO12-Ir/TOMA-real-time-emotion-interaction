后台检测声音，如果有声音开始录音，安静一段时间后截断，或是时间超过一段时间后自动结束。接着存储下来语音，用存储的语音进行SER分析，获得VAD三维和标签情绪。然后将这些情绪数据进入自制response classifier，获得分类后的宠物反应。修改宠物反应变量，并被一直运行的pygame检测，并更换表情图片



项目流程说明（System Pipeline）

TOMA 采用实时音频流驱动的情绪感知与反馈机制，整体流程如下：

1. 实时音频监听与录制控制

后台持续监听麦克风输入，通过音量阈值检测是否存在语音活动：

当检测到音量超过阈值时，开始录音；

若在录音过程中持续一段时间未检测到语音（静音），则自动截断；

若录音时长超过预设上限，也会强制结束录音。

该机制确保只保存有效语音片段，避免长时间空录或无意义音频。

2. 语音片段存储

被截断的语音片段会被临时保存为音频文件，用于后续分析。
录音完成后，系统立即进入情绪识别阶段，同时后台可继续监听新的语音输入。

3. 语音情绪识别（SER）

保存的语音片段会送入预训练的语音情绪识别模型进行推理，输出包括：

离散情绪标签（如 Neutral、Happy、Sad 等）；

连续的 VAD 情绪向量：

Valence（情绪正负性）

Arousal（唤醒程度）

Dominance（控制感）

此外，系统对 VAD 输出进行了时间平滑与冷却处理，以减少瞬时波动带来的不稳定响应。

4. 自定义 Response Classifier

SER 输出的情绪信息并不会直接决定宠物行为，而是作为输入特征，送入一个自制的 response classifier：

该分类器基于手动标注的小规模数据训练；

输入为情绪标签与 VAD 数值；

输出为离散的宠物反应类别（如 calm / happy / sad / fear）。

这一层实现了从“情绪感知”到“行为决策”的映射。

5. 宠物状态更新与视觉反馈

response classifier 的输出会更新一个全局的宠物反应状态变量。
该状态被一个持续运行的 pygame 渲染循环实时读取，并用于：

切换对应的表情图片；

更新宠物的视觉反馈状态。

从用户视角来看，宠物会根据说话时的语气变化，实时表现出不同的情绪反应。